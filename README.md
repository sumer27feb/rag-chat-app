# ğŸ§  RAG Chat App

A **Retrieval-Augmented Generation (RAG)** powered chat application that allows users to upload files (PDF, text, etc.) and ask **contextual questions** about their content. Built as part of my learning journey in **AI, backend engineering, and full-stack development**, this project demonstrates how to combine **modern AI frameworks with solid backend architecture**.

---

## ğŸš€ Features

- ğŸ“‚ **File Upload & Ingestion** â€“ Users can upload files which are processed and stored in a vector database (FAISS).
- ğŸ” **Context-Aware QA** â€“ Uses RAG pipeline to fetch relevant chunks and generate accurate answers with LLMs.
- ğŸ› ï¸ **Frontend** â€“ Built with **React + TypeScript** for a clean and responsive interface.
- âš¡ **Backend** â€“ **FastAPI** for APIs, document ingestion, and orchestrating retrieval/generation.
- ğŸ§© **LangChain Integration** â€“ Chunking, embeddings, retrieval pipeline.
- ğŸ³ **Dockerized Deployment** â€“ Portable and production-ready setup with Docker & docker-compose.
- ğŸ”‘ **Secure Config Management** â€“ Environment variables via `.env` (ignored in git).
- ğŸ“Š **Extensible Architecture** â€“ Can plug in other vector DBs (Pinecone, Weaviate, ChromaDB).

---

## ğŸ—ï¸ Tech Stack

**Frontend**

- Next.js / React (TypeScript)
- Tailwind CSS (for styling)

**Backend**

- FastAPI (Python)
- LangChain
- FAISS (vector search)

**AI / LLM**

- OpenAI API (can swap with other LLM providers)
- Custom embeddings pipeline

**DevOps**

- Docker, docker-compose
- GitHub for version control

---

## ğŸ“‚ Project Structure

```
rag-chat-app/
â”‚â”€â”€ frontend/         # React + TypeScript frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tsconfig.json
â”‚
â”‚â”€â”€ backend/          # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py           # Entry point
â”‚   â”‚   â”œâ”€â”€ routes/           # API endpoints
â”‚   â”‚   â”œâ”€â”€ services/         # RAG logic
â”‚   â”‚   â””â”€â”€ utils/            # Helpers
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”‚
â”‚â”€â”€ docker/           # Docker & compose configs
â”‚â”€â”€ .gitignore
â”‚â”€â”€ README.md
```

---

## âš™ï¸ Setup & Installation

### 1. Clone the repo

```bash
git clone https://github.com/sumer27feb/rag-chat-app.git
cd rag-chat-app
```

### 2. Frontend setup

```bash
cd frontend
npm install
npm run dev
```

### 3. Backend setup

```bash
cd backend
python -m venv venv
source venv/bin/activate   # on Linux/Mac
venv\Scripts\activate      # on Windows

pip install -r requirements.txt
uvicorn app.main:app --reload
```

---

## ğŸ§ª Usage

1. Upload a PDF/text file through the UI.
2. The backend ingests the file, chunks it, and stores embeddings in FAISS.
3. Ask questions in the chat â€“ the app retrieves relevant chunks and passes them to the LLM.
4. Get **context-aware answers** instantly.

---

## ğŸ“– Roadmap

- [ ] Authentication with JWT
- [ ] Support for multiple users & sessions
- [ ] Add Redis caching layer
- [ ] Integrate Sentry for monitoring
- [ ] Extend to support images/audio in RAG
- [ ] Deploy on cloud (Render / Vercel / AWS)

---

## ğŸ§‘â€ğŸ’» About the Author

ğŸ‘‹ Hi, Iâ€™m **Sumer Dev Singh** â€“ a full-stack developer & 2025 CSE graduate.  
I love building **backend architectures, AI-powered applications, and experimenting with LLMs**.

- ğŸŒ [Portfolio](https://sumer-dev-singh-portfolio.vercel.app)
- ğŸ’» [GitHub](https://github.com/sumer27feb)
- ğŸ’¼ [LinkedIn](https://www.linkedin.com/in/sumer-dev-singh-35870a234)

---

## âš ï¸ Disclaimer

This project is **work in progress** ğŸš§. Expect frequent updates and refactors.  
Itâ€™s primarily a **learning and showcase project**, but will evolve into a fully-fledged AI assistant over time.

---
